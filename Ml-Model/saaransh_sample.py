# -*- coding: utf-8 -*-
"""Saaransh sample.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3SLYrbVIGObtxlrmnJfhW0H_iDOBpN4
"""

# =====================
# 0. Setup Cell (Install Dependencies + Upload Dataset)
# =====================
# Run this cell first in Google Colab (with GPU enabled: Runtime -> Change runtime type -> GPU)

# Upgrade pip
!pip install --upgrade pip

# Install PyTorch with GPU (CUDA 12.1 version)
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install HuggingFace, ML, and NLP dependencies
!pip install transformers datasets scikit-learn pandas matplotlib tqdm regex pyttsx3

# Optional: install espeak (for pyttsx3 voice on Linux/Colab)
!apt-get install -y espeak

# Upload dataset (EnglishPoliticalTweets.csv)
from google.colab import files
uploaded = files.upload()

import pandas as pd
# Load uploaded dataset
for filename in uploaded.keys():
    df = pd.read_csv("/content/EnglishPoliticalTweets.csv")

print("Dataset loaded successfully! Shape:", df.shape)
df.head()

# =====================
# 2. Preprocessing function (with label normalization)
# =====================
import re

# Normalize labels: lowercase and map to main categories
df['Sentiment'] = df['Sentiment'].str.lower()

label_map = {
    "positive": 0,
    "POSITIVE": 0,
    "negative": 1,
    "NEGATIVE": 1,
    "neutral": 2,
    "NEUTRAL": 2,
    "suggestion": 2   # treating suggestion as neutral (can adjust if needed)
}

df['label'] = df['Sentiment'].map(label_map)

# Drop rows with unmapped labels (if any)
df = df.dropna(subset=['label'])

# Clean text function
def clean_text(text):
    text = re.sub(r'http\\S+', '', str(text))       # remove links
    text = re.sub(r'@\\w+', '', text)               # remove mentions
    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)      # remove special chars
    return text.lower().strip()

# Add clean_text column
df['clean_text'] = df['OriginalTweet'].apply(clean_text)

print("Class distribution after mapping:")
print(df['label'].value_counts())

df.head()



# =====================
# 2. Dataset Preparation for BERT
# =====================

from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizer
import torch

# Split dataset
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['clean_text'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenization
def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

train_dataset = Dataset.from_dict({"text": train_texts, "label": train_labels})
test_dataset = Dataset.from_dict({"text": test_texts, "label": test_labels})

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Cast label column to torch.long
train_dataset = train_dataset.map(lambda x: {'label': x['label'].long()})
test_dataset = test_dataset.map(lambda x: {'label': x['label'].long()})

# =====================
# 3. BERT Model Training
# =====================

import os, warnings
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, classification_report

# Disable W&B logging + suppress warnings
os.environ["WANDB_DISABLED"] = "true"
warnings.filterwarnings("ignore")

# Load model (3-class classification)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

# Set problem type to single_label_classification for correct loss calculation
model.config.problem_type = "single_label_classification"

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",    # fixed correct arg name
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    report_to="none"   # fully disable wandb/tensorboard logging
)

# Compute metrics (accuracy + classification report at eval)
def compute_metrics(eval_pred):
    labels = eval_pred.label_ids
    preds = eval_pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# =====================
# 4. Evaluation
# =====================

preds_output = trainer.predict(test_dataset)
y_true = preds_output.label_ids
y_pred = preds_output.predictions.argmax(-1)

print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=["negative", "neutral", "positive"]))

# =====================
# 5. Prediction on New Tweets
# =====================

def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = model(**inputs).logits
    pred = torch.argmax(logits, dim=1).item()
    mapping = {0: "negative", 1: "neutral", 2: "positive"}
    return mapping[pred]

# Example
sample_text = "I love how this policy is changing lives!"
print("Tweet:", sample_text)
print("Predicted Sentiment:", predict_sentiment(sample_text))

# =====================
# 7. Visualization (Pie, Bar, WordCloud)
# =====================

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Sentiment distribution
sentiment_counts = df['Sentiment'].value_counts()

# --- Pie Chart ---
plt.figure(figsize=(6,6))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=['red','gold','green'])
plt.title("Sentiment Distribution (Pie Chart)")
plt.show()

# --- Bar Chart ---
plt.figure(figsize=(6,4))
sentiment_counts.plot(kind='bar', color=['red','gold','green'])
plt.title("Sentiment Distribution (Bar Chart)")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

# --- WordClouds ---
for sentiment in ['positive', 'neutral', 'negative']:
    text = " ".join(df[df['Sentiment']==sentiment]['clean_text'].tolist())
    if text.strip():  # avoid empty
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
        plt.figure(figsize=(10,5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f"WordCloud for {sentiment} tweets")
        plt.show()